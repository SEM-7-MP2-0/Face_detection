{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceCounter:\n",
    "    def __init__(self):\n",
    "        self.mp_face_detection = mp.solutions.face_detection\n",
    "        self.face_detection = self.mp_face_detection.FaceDetection()\n",
    "\n",
    "    def count_faces(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        image_np = self._pil_to_numpy(image)\n",
    "\n",
    "        results = self.face_detection.process(image_np)\n",
    "        num_faces = len(results.detections) if results.detections else 0\n",
    "\n",
    "        return num_faces\n",
    "\n",
    "    def _pil_to_numpy(self, image):\n",
    "        return np.array(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./images/2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_counter = FaceCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "total_faces = face_counter.count_faces(image_path)\n",
    "print(total_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options = python.BaseOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = vision.FaceDetectorOptions(base_options=base_options, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExternalFile must specify at least one of 'file_content', 'file_name', 'file_pointer_meta' or 'file_descriptor_meta'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    detector = vision.FaceDetector.create_from_options(options)\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "imagePath = './images/4.jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(imagePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 1599, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 1599)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "face_classifier_2 = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_alt.xml\"\n",
    ")\n",
    "\n",
    "face_classifier_3 = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_alt2.xml\"\n",
    ")\n",
    "\n",
    "face_classifier_4 = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_alt_tree.xml\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = face_classifier.detectMultiScale(\n",
    "    img, scaleFactor=1.1, minNeighbors=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 638,  130,   88,   88],\n",
       "       [ 888,  235,   72,   72],\n",
       "       [ 588,  166,   33,   33],\n",
       "       [ 623,  285,   89,   89],\n",
       "       [1493,  295,   78,   78],\n",
       "       [ 829,  194,   57,   57],\n",
       "       [ 129,  215,   34,   34],\n",
       "       [ 531,  211,   50,   50],\n",
       "       [1241,  214,   47,   47],\n",
       "       [ 432,  197,   78,   78],\n",
       "       [1079,  222,   70,   70],\n",
       "       [ 989,  225,   52,   52]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y, w, h) in face:\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "\n",
    "cv2.imshow(\"Face\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_rgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\face.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/salman/OneDrive/Desktop/SEM7/Project-2/Face_Detection/face.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/salman/OneDrive/Desktop/SEM7/Project-2/Face_Detection/face.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/salman/OneDrive/Desktop/SEM7/Project-2/Face_Detection/face.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(img_rgb)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/salman/OneDrive/Desktop/SEM7/Project-2/Face_Detection/face.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_rgb' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def detect_faces(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = self.transforms(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(image_tensor)\n",
    "        \n",
    "        return predictions[0]\n",
    "\n",
    "    def visualize_detections(self, image_path, predictions):\n",
    "        image = Image.open(image_path)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        for score, label, box in zip(predictions[\"scores\"], predictions[\"labels\"], predictions[\"boxes\"]):\n",
    "            if score > 0.7:  # You can adjust this threshold\n",
    "                box = [round(i.item()) for i in box]\n",
    "                draw.rectangle(box, outline=\"red\", width=2)\n",
    "        \n",
    "        image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "face_detector = FaceDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = face_detector.detect_faces(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector.visualize_detections(image_path, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_faces = len(predictions[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(total_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './images/2.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.cfg = get_cfg()\n",
    "        self.cfg.merge_from_file(\"configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Change to the appropriate config file\n",
    "        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "        self.cfg.MODEL.WEIGHTS = \"path/to/model/weights\"  # Change to the appropriate path\n",
    "        self.predictor = DefaultPredictor(self.cfg)\n",
    "\n",
    "    def detect_faces(self, image_path):\n",
    "        im = cv2.imread(image_path)\n",
    "        outputs = self.predictor(im)\n",
    "        return outputs\n",
    "\n",
    "    def visualize_detections(self, image_path, outputs):\n",
    "        im = cv2.imread(image_path)\n",
    "        v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "        v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        cv2.imshow(\"Detection\", v.get_image()[:, :, ::-1])\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Config file 'configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml' does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m face_detector \u001b[39m=\u001b[39m FaceDetector()\n",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m, in \u001b[0;36mFaceDetector.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg \u001b[39m=\u001b[39m get_cfg()\n\u001b[1;32m---> 12\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg\u001b[39m.\u001b[39;49mmerge_from_file(\u001b[39m\"\u001b[39;49m\u001b[39mconfigs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# Change to the appropriate config file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mROI_HEADS\u001b[39m.\u001b[39mSCORE_THRESH_TEST \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mWEIGHTS \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpath/to/model/weights\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Change to the appropriate path\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\detectron2\\config\\config.py:45\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[1;34m(self, cfg_filename, allow_unsafe)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_from_file\u001b[39m(\u001b[39mself\u001b[39m, cfg_filename: \u001b[39mstr\u001b[39m, allow_unsafe: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m    Load content from the given config file and merge it into self.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m        allow_unsafe: allow unsafe yaml syntax\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[39massert\u001b[39;00m PathManager\u001b[39m.\u001b[39misfile(cfg_filename), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConfig file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcfg_filename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     loaded_cfg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_yaml_with_base(cfg_filename, allow_unsafe\u001b[39m=\u001b[39mallow_unsafe)\n\u001b[0;32m     47\u001b[0m     loaded_cfg \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)(loaded_cfg)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Config file 'configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml' does not exist!"
     ]
    }
   ],
   "source": [
    "face_detector = FaceDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
