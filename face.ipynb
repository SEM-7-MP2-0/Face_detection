{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceCounter:\n",
    "    def __init__(self):\n",
    "        self.mp_face_detection = mp.solutions.face_detection\n",
    "        self.face_detection = self.mp_face_detection.FaceDetection()\n",
    "\n",
    "    def count_faces(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        image_np = self._pil_to_numpy(image)\n",
    "\n",
    "        results = self.face_detection.process(image_np)\n",
    "        num_faces = len(results.detections) if results.detections else 0\n",
    "\n",
    "        return num_faces\n",
    "\n",
    "    def _pil_to_numpy(self, image):\n",
    "        return np.array(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./images/2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_counter = FaceCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "total_faces = face_counter.count_faces(image_path)\n",
    "print(total_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options = python.BaseOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = vision.FaceDetectorOptions(base_options=base_options, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExternalFile must specify at least one of 'file_content', 'file_name', 'file_pointer_meta' or 'file_descriptor_meta'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    detector = vision.FaceDetector.create_from_options(options)\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "imagePath = './images/4.jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(imagePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 1599, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 1599)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "face_classifier_2 = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_alt.xml\"\n",
    ")\n",
    "\n",
    "face_classifier_3 = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_alt2.xml\"\n",
    ")\n",
    "\n",
    "face_classifier_4 = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_alt_tree.xml\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = face_classifier_3.detectMultiScale(\n",
    "    gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1010,  168,   44,   44],\n",
       "       [1494,  296,   77,   77],\n",
       "       [ 128,  216,   32,   32],\n",
       "       [1243,  216,   43,   43],\n",
       "       [1360,  229,   41,   41],\n",
       "       [ 988,  225,   54,   54],\n",
       "       [1079,  219,   76,   76],\n",
       "       [ 887,  234,   74,   74]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y, w, h) in face:\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib_inline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m plt\u001b[39m.\u001b[39;49mfigure(figsize\u001b[39m=\u001b[39;49m(\u001b[39m20\u001b[39;49m,\u001b[39m10\u001b[39;49m))\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mimshow(img_rgb)\n\u001b[0;32m      7\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[0;32m    449\u001b[0m     warn_deprecated(\n\u001b[0;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\matplotlib\\pyplot.py:840\u001b[0m, in \u001b[0;36mfigure\u001b[1;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(allnums) \u001b[39m==\u001b[39m max_open_warning \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    831\u001b[0m     _api\u001b[39m.\u001b[39mwarn_external(\n\u001b[0;32m    832\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMore than \u001b[39m\u001b[39m{\u001b[39;00mmax_open_warning\u001b[39m}\u001b[39;00m\u001b[39m figures have been opened. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFigures created through the pyplot interface \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    837\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConsider using `matplotlib.pyplot.close()`.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    838\u001b[0m         \u001b[39mRuntimeWarning\u001b[39;00m)\n\u001b[1;32m--> 840\u001b[0m manager \u001b[39m=\u001b[39m new_figure_manager(\n\u001b[0;32m    841\u001b[0m     num, figsize\u001b[39m=\u001b[39mfigsize, dpi\u001b[39m=\u001b[39mdpi,\n\u001b[0;32m    842\u001b[0m     facecolor\u001b[39m=\u001b[39mfacecolor, edgecolor\u001b[39m=\u001b[39medgecolor, frameon\u001b[39m=\u001b[39mframeon,\n\u001b[0;32m    843\u001b[0m     FigureClass\u001b[39m=\u001b[39mFigureClass, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    844\u001b[0m fig \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mfigure\n\u001b[0;32m    845\u001b[0m \u001b[39mif\u001b[39;00m fig_label:\n",
      "File \u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\matplotlib\\pyplot.py:383\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_figure_manager\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    382\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m     _warn_if_gui_out_of_main_thread()\n\u001b[0;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_backend_mod()\u001b[39m.\u001b[39mnew_figure_manager(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\matplotlib\\pyplot.py:361\u001b[0m, in \u001b[0;36m_warn_if_gui_out_of_main_thread\u001b[1;34m()\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_warn_if_gui_out_of_main_thread\u001b[39m():\n\u001b[0;32m    360\u001b[0m     warn \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mif\u001b[39;00m _get_required_interactive_framework(_get_backend_mod()):\n\u001b[0;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(threading, \u001b[39m'\u001b[39m\u001b[39mget_native_id\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    363\u001b[0m             \u001b[39m# This compares native thread ids because even if Python-level\u001b[39;00m\n\u001b[0;32m    364\u001b[0m             \u001b[39m# Thread objects match, the underlying OS thread (which is what\u001b[39;00m\n\u001b[0;32m    365\u001b[0m             \u001b[39m# really matters) may be different on Python implementations with\u001b[39;00m\n\u001b[0;32m    366\u001b[0m             \u001b[39m# green threads.\u001b[39;00m\n\u001b[0;32m    367\u001b[0m             \u001b[39mif\u001b[39;00m threading\u001b[39m.\u001b[39mget_native_id() \u001b[39m!=\u001b[39m threading\u001b[39m.\u001b[39mmain_thread()\u001b[39m.\u001b[39mnative_id:\n",
      "File \u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\matplotlib\\pyplot.py:208\u001b[0m, in \u001b[0;36m_get_backend_mod\u001b[1;34m()\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39mEnsure that a backend is selected and return it.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[39mThis is currently private, but may be made public in the future.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m _backend_mod \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[39m# Use rcParams._get(\"backend\") to avoid going through the fallback\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     \u001b[39m# logic (which will (re)import pyplot and then call switch_backend if\u001b[39;00m\n\u001b[0;32m    207\u001b[0m     \u001b[39m# we need to resolve the auto sentinel)\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     switch_backend(rcParams\u001b[39m.\u001b[39;49m_get(\u001b[39m\"\u001b[39;49m\u001b[39mbackend\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m _backend_mod\n",
      "File \u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\matplotlib\\pyplot.py:271\u001b[0m, in \u001b[0;36mswitch_backend\u001b[1;34m(newbackend)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39m# have to escape the switch on access logic\u001b[39;00m\n\u001b[0;32m    269\u001b[0m old_backend \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(rcParams, \u001b[39m'\u001b[39m\u001b[39mbackend\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m backend_mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\n\u001b[0;32m    272\u001b[0m     cbook\u001b[39m.\u001b[39;49m_backend_module_name(newbackend))\n\u001b[0;32m    274\u001b[0m required_framework \u001b[39m=\u001b[39m _get_required_interactive_framework(backend_mod)\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m required_framework \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:972\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:984\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib_inline'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def detect_faces(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = self.transforms(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(image_tensor)\n",
    "        \n",
    "        return predictions[0]\n",
    "\n",
    "    def visualize_detections(self, image_path, predictions):\n",
    "        image = Image.open(image_path)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        for score, label, box in zip(predictions[\"scores\"], predictions[\"labels\"], predictions[\"boxes\"]):\n",
    "            if score > 0.7:  # You can adjust this threshold\n",
    "                box = [round(i.item()) for i in box]\n",
    "                draw.rectangle(box, outline=\"red\", width=2)\n",
    "        \n",
    "        image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "face_detector = FaceDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = face_detector.detect_faces(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector.visualize_detections(image_path, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_faces = len(predictions[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(total_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './images/2.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self):\n",
    "        self.cfg = get_cfg()\n",
    "        self.cfg.merge_from_file(\"configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Change to the appropriate config file\n",
    "        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "        self.cfg.MODEL.WEIGHTS = \"path/to/model/weights\"  # Change to the appropriate path\n",
    "        self.predictor = DefaultPredictor(self.cfg)\n",
    "\n",
    "    def detect_faces(self, image_path):\n",
    "        im = cv2.imread(image_path)\n",
    "        outputs = self.predictor(im)\n",
    "        return outputs\n",
    "\n",
    "    def visualize_detections(self, image_path, outputs):\n",
    "        im = cv2.imread(image_path)\n",
    "        v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(self.cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "        v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        cv2.imshow(\"Detection\", v.get_image()[:, :, ::-1])\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Config file 'configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml' does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m face_detector \u001b[39m=\u001b[39m FaceDetector()\n",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m, in \u001b[0;36mFaceDetector.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg \u001b[39m=\u001b[39m get_cfg()\n\u001b[1;32m---> 12\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg\u001b[39m.\u001b[39;49mmerge_from_file(\u001b[39m\"\u001b[39;49m\u001b[39mconfigs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# Change to the appropriate config file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mROI_HEADS\u001b[39m.\u001b[39mSCORE_THRESH_TEST \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mWEIGHTS \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpath/to/model/weights\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Change to the appropriate path\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\salman\\OneDrive\\Desktop\\SEM7\\Project-2\\Face_Detection\\env\\lib\\site-packages\\detectron2\\config\\config.py:45\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[1;34m(self, cfg_filename, allow_unsafe)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_from_file\u001b[39m(\u001b[39mself\u001b[39m, cfg_filename: \u001b[39mstr\u001b[39m, allow_unsafe: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m    Load content from the given config file and merge it into self.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m        allow_unsafe: allow unsafe yaml syntax\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[39massert\u001b[39;00m PathManager\u001b[39m.\u001b[39misfile(cfg_filename), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConfig file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcfg_filename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     loaded_cfg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_yaml_with_base(cfg_filename, allow_unsafe\u001b[39m=\u001b[39mallow_unsafe)\n\u001b[0;32m     47\u001b[0m     loaded_cfg \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)(loaded_cfg)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Config file 'configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml' does not exist!"
     ]
    }
   ],
   "source": [
    "face_detector = FaceDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
